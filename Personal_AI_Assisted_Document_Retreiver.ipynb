{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c6be6d80-00b9-4cfd-b812-fcfd9f5c2a3e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Installing the Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da730f2b-8119-4d54-8239-b5fcc54465b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install langchain langchain-community openai tiktoken chromadb docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b722965-fbbe-4084-8ba7-678248be31b7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install pytesseract pillow pdf2image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d8fe26eb-8458-4106-b763-007a20d934b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28500fd1-da3c-4af3-bf05-f8f9cee4eaa7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab2db702-8d19-4991-85bd-a52ef7b522ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### OpenAI Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84b9f9a0-a831-422d-9a96-2b181af010fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7aed9951-71b2-4aad-a0a7-9742fca8e42b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Load Word Documents with Text and Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2f13b163-76d1-4c0a-ab94-365c84e8d1ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68 documents loaded\n",
      "Extract, Transform & Load\n",
      "\n",
      "Step 1 – Extract the data (All the structured, semi-structured & unstruct\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_community.document_loaders import Docx2txtLoader\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import docx\n",
    "import io\n",
    "\n",
    "doc_dir = \"/Volumes/workspace/default/ai_assistant_rag_documents\"\n",
    "\n",
    "documents = []\n",
    "for filename in os.listdir(doc_dir):\n",
    "    if filename.endswith(\".docx\"):\n",
    "        file_path = os.path.join(doc_dir, filename)\n",
    "        # Load text from the Word Document\n",
    "        loader = Docx2txtLoader(file_path)\n",
    "        documents.extend(loader.load())\n",
    "\n",
    "        # Using OCR to extract text from images in the Word Document\n",
    "        doc = docx.Document(file_path)\n",
    "        for i,rel in enumerate(doc.part.rels.values()):\n",
    "            if \"image\" in rel.target_ref:\n",
    "                image_data = rel.target_part.blob\n",
    "                image = Image.open(io.BytesIO(image_data))\n",
    "                text = pytesseract.image_to_string(image)\n",
    "                if text.strip():\n",
    "                        documents.append(\n",
    "                            Document(\n",
    "                                page_content=text,\n",
    "                                metadata={\"source\": file_path, \"page\": f\"image_{i}\"}\n",
    "                                    )\n",
    "                                )\n",
    "\n",
    "print(len(documents), \"documents loaded\")\n",
    "print(documents[0].page_content[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c05aa795-dbbe-483e-a807-0c182e7cb0d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Split Into Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8be08cca-771e-4a01-a3db-6fb3476f7ca1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 chunks of document created\n",
      "page_content='Extract, Transform & Load\n",
      "\n",
      "Step 1 – Extract the data (All the structured, semi-structured & unstructured )\n",
      "\n",
      "Step 2 – Transform the data (Take the data values that are required, clean the data, convert data into desired format)\n",
      "\n",
      "Step 3 – Load the data (Can be either ADLS Gen 2 Account or can be SQL Warehouse like Synapse)\n",
      "\n",
      "\n",
      "\n",
      "So, the ETL Process includes:\n",
      "\n",
      "Extraction -> Staging Area -> Transform -> Load -> Data Warehouse -> Analytics\n",
      "\n",
      "\n",
      "\n",
      "Data Warehouse Architecture\n",
      "\n",
      "Data Source -> Data Staging (ETL) -> Data Storage (Data Warehouse & Data Marts)  -> Data Presentation (Data Analytics, Reporting & Data Mining)\n",
      "\n",
      "\n",
      "\n",
      "Azure Data Factory (ADF)\n",
      "\n",
      "This is a cloud based ETL and data integration service.\n",
      "\n",
      "You can create data-driven workflows that can be used for orchestrating data movement.\n",
      "\n",
      "You can also transform data at scale.\n",
      "\n",
      "You can connect to a variety of data sources as the source and destination.\n",
      "\n",
      "ADLS Gen2 (Source) ------------ Data Factory ----------- Azure Synapse Analytics (Destination)' metadata={'source': '/Volumes/workspace/default/ai_assistant_rag_documents/Azure Data Factory (ADF) Notes.docx'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "chunks = splitter.split_documents(documents)\n",
    "\n",
    "print(len(chunks), \"chunks of document created\")\n",
    "print(chunks[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b3352bf-7509-4b7b-8c96-93d04dac77bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Create Vector Store and Vector Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b5248b4a-a2af-4521-86be-a0a2d081e4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "per_dir = \"/Workspace/Users/jaggiakshat@gmail.com/rag_chromastoreV2\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model = \"text-embedding-ada-002\", openai_api_key = OPENAI_API_KEY)\n",
    "vector_store = Chroma.from_documents(chunks, embeddings, persist_directory=per_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7b196d4-4cf5-4925-bf2f-29caafcd4fbe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "You can re-use the Chroma Vector Store stored in the persistent directory like below:\n",
    "<br> </br>\n",
    "\n",
    "```python\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "persist_path = \"/Workspace/rag_chromastore\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Reload persisted vector store\n",
    "vector_store = Chroma(\n",
    "    persist_directory=persist_path,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "print(\"Reloaded vector store with\", vector_store._collection.count(), \"documents\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1d18b10-dbdf-4286-8d5c-206563b7c557",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Build RAG Retriever + Chain "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9b1e97d-45b5-4337-ad26-3a5d6da30f08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model = \"gpt-4o-mini\", temperature = 0, openai_api_key = OPENAI_API_KEY)\n",
    "\n",
    "retreiver = vector_store.as_retriever(search_type = \"similarity\", search_kwargs = {\"k\": 5}) \n",
    "# k here means the number of documents to return from the vector store after a similarity search\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=llm, \n",
    "                                       chain_type=\"stuff\", # Concatenates the retrieved 5 chunks with user query\n",
    "                                       retriever=retreiver, \n",
    "                                       return_source_documents=True\n",
    "                                       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5dd81744-e615-4f97-81a4-a174c8ede6b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Asking Questions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eb74c39a-da22-4ce2-9318-c9b41543dbae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  The ETL Process stands for Extract, Transform, and Load. It consists of three main steps:\n",
      "\n",
      "1. **Extract**: This step involves extracting data from various sources, which can include structured, semi-structured, and unstructured data.\n",
      "\n",
      "2. **Transform**: In this step, the extracted data is transformed to meet the desired format. This includes cleaning the data, taking only the required data values, and converting the data into a suitable format for analysis.\n",
      "\n",
      "3. **Load**: Finally, the transformed data is loaded into a destination, which can be an Azure Data Lake Storage (ADLS) Gen 2 account or a SQL Warehouse like Azure Synapse Analytics.\n",
      "\n",
      "The overall ETL process can be visualized as follows: Extraction -> Staging Area -> Transform -> Load -> Data Warehouse -> Analytics.\n",
      "Source Document Names:\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Data Factory (ADF) Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure_Event_Hub_&_Stream_Analytics.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure_Event_Hub_&_Stream_Analytics.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n"
     ]
    }
   ],
   "source": [
    "query = \"Can you please explain to me what is ETL Process?\"\n",
    "result = qa_chain({\"query\":query})\n",
    "\n",
    "print(\"Answer: \", result[\"result\"])\n",
    "print(\"Source Document Names:\")\n",
    "for doc in result[\"source_documents\"]:\n",
    "    print(doc.metadata.get(\"source\", \"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e216d716-960d-47de-87d2-e7d3de35b4c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  Azure Synapse Analytics is a unified analytics platform that allows you to analyze large amounts of data using various services, including both SQL-based and Apache Spark-based pools. It integrates data from different sources and provides the necessary infrastructure for data warehousing and big data analytics. Key components of Azure Synapse include Synapse SQL for hosting SQL Data Warehouses, Apache Spark for big data processing, and data integration features similar to Azure Data Factory.\n",
      "\n",
      "As for the difference between Azure Synapse and Databricks, I don't know.\n",
      "Source Document Names:\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n"
     ]
    }
   ],
   "source": [
    "query2 = \"What is Azure Synapse Analytics? Also, What is the difference between Synapse and Databricks?\"\n",
    "result2 = qa_chain({\"query\":query2})\n",
    "\n",
    "print(\"Answer: \", result2[\"result\"])\n",
    "print(\"Source Document Names:\")\n",
    "for doc in result2[\"source_documents\"]:\n",
    "    print(doc.metadata.get(\"source\", \"Unknown\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1d78482a-17e8-40f4-bdde-7ee4178b4ce7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:  ACID solves the following problems:\n",
      "\n",
      "1. Streamlined Data Append: It simplifies and improves data appending, making it efficient even with concurrent writes.\n",
      "\n",
      "2. Simplified Data Modification: It ensures data modification is straightforward while maintaining data consistency.\n",
      "\n",
      "3. Data Integrity Through Job Failures: It prevents data inconsistencies due to job failures, thereby maintaining data integrity.\n",
      "\n",
      "4. Support for Real-time Operations: It serves as a robust data source and sink for real-time and streaming operations.\n",
      "\n",
      "5. Efficient Historical Data Version Management: It offers time travel for accessing historical data versions, ensuring cost-effectiveness based on specific use cases and alternatives. \n",
      "\n",
      "6. Guarantees Data Consistency: It ensures data consistency and quality, avoiding issues with concurrent writes. \n",
      "\n",
      "7. Supports Upserts (MERGE): It allows for efficient updates through upserts or merges.\n",
      "Source Document Names:\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Data Ingestion with Delta Lake Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n",
      "/Volumes/workspace/default/ai_assistant_rag_documents/Azure Synapse Notes.docx\n"
     ]
    }
   ],
   "source": [
    "query3 = \"What are the problems solved by ACID?\"\n",
    "result3 = qa_chain({\"query\":query3})\n",
    "\n",
    "print(\"Answer: \", result3[\"result\"])\n",
    "print(\"Source Document Names:\")\n",
    "for doc in result3[\"source_documents\"]:\n",
    "    print(doc.metadata.get(\"source\", \"Unknown\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dfa3b5c6-9d77-4b40-9fbc-44ac0effc5da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Let's try to create an Enhancement where when the result doesn't have any fetched documents or it has the words \"I don't know\", it will call the general model, in our case \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7098bcfd-3a45-4f40-aaab-b488e1680c91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Fallback to Open Domain LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "927c0062-f516-4c30-84dc-10c60645f952",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def smart_qa(query):\n",
    "    # Try RAG first\n",
    "    result = qa_chain({\"query\": query})\n",
    "    \n",
    "    if \"I don't know\" in result[\"result\"] or not result[\"source_documents\"]:\n",
    "        # Fallback to general LLM knowledge\n",
    "        print(\"Not found in docs. Using general LLM knowledge.\")\n",
    "        return llm.predict(f\"Answer this using general knowledge: {query}\")\n",
    "    \n",
    "    return result[\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3f508ed-0a34-49d6-8a7f-3e75c2e5c5af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found in docs. Using general LLM knowledge.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Azure Synapse Analytics is a cloud-based integrated analytics service provided by Microsoft Azure. It combines big data and data warehousing capabilities, allowing organizations to analyze large volumes of data from various sources. Synapse enables users to ingest, prepare, manage, and serve data for business intelligence and analytics. It provides a unified experience for data integration, data warehousing, and big data analytics, allowing users to run complex queries and perform analytics using both SQL and Spark.\\n\\nKey features of Azure Synapse Analytics include:\\n\\n1. **Data Integration**: It allows users to connect to various data sources, both on-premises and in the cloud, and integrate them into a single analytics platform.\\n2. **Data Warehousing**: Synapse provides a powerful SQL-based data warehousing solution that can handle large datasets and complex queries.\\n3. **Big Data Analytics**: It supports big data processing using Apache Spark, enabling users to analyze unstructured and semi-structured data.\\n4. **Serverless and Provisioned Resources**: Users can choose between serverless on-demand queries and provisioned resources for more consistent performance.\\n5. **Integrated Development Environment**: Synapse Studio provides a collaborative workspace for data engineers, data scientists, and business analysts to work together.\\n\\n### Difference Between Azure Synapse Analytics and Databricks\\n\\nWhile both Azure Synapse Analytics and Azure Databricks are cloud-based analytics services offered by Microsoft Azure, they serve different purposes and have distinct features:\\n\\n1. **Primary Focus**:\\n   - **Azure Synapse Analytics**: Primarily focuses on data warehousing and integrated analytics. It combines data integration, data warehousing, and big data analytics into a single platform.\\n   - **Azure Databricks**: Primarily focuses on big data processing and machine learning. It is built on Apache Spark and is designed for data engineering, data science, and machine learning workflows.\\n\\n2. **Data Processing**:\\n   - **Azure Synapse**: Supports both SQL-based queries and Spark-based processing, allowing users to work with structured and unstructured data.\\n   - **Azure Databricks**: Optimized for Spark-based processing, making it ideal for large-scale data processing, real-time analytics, and machine learning tasks.\\n\\n3. **User Interface**:\\n   - **Azure Synapse**: Offers Synapse Studio, which provides a unified workspace for data integration, data exploration, and analytics.\\n   - **Azure Databricks**: Provides a collaborative workspace with notebooks for data scientists and engineers to write code, visualize data, and share insights.\\n\\n4. **Integration**:\\n   - **Azure Synapse**: Integrates with various Azure services, including Azure Data Lake Storage, Azure Machine Learning, and Power BI, to provide a comprehensive analytics solution.\\n   - **Azure Databricks**: Also integrates well with Azure services but is particularly strong in machine learning and data science workflows, often used in conjunction with Azure Machine Learning.\\n\\n5. **Use Cases**:\\n   - **Azure Synapse**: Best suited for organizations looking for a comprehensive analytics solution that includes data warehousing, reporting, and business intelligence.\\n   - **Azure Databricks**: Ideal for organizations focused on big data analytics, data engineering, and machine learning projects.\\n\\nIn summary, while both Azure Synapse Analytics and Azure Databricks are powerful tools for data analytics, they cater to different needs and use cases within the data ecosystem.'"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_qa(\"What is Azure Synapse Analytics? Also, What is the difference between Synapse and Databricks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "414e14cb-0fd0-4bdd-b1d2-b1ddb681a195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found in docs. Using general LLM knowledge.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'As of my last knowledge update in October 2023, the Prime Minister of India is Narendra Modi. He has been in office since May 26, 2014. Please verify with up-to-date sources, as political positions can change.'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_qa(\"Who is the Prime Minister of India?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a8fa643-6387-4bf1-9517-5684091a4d6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Gradio Mini Chatbot App (Not Possible to run on Databricks Free Edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dd66920-f022-4f96-a138-d22765cb2cf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Could Not run Gradio Mini App in Databricks Free Edition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "802df2d0-2b18-4878-8f95-c1426f668e2e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# def answer_question(user_input):\n",
    "#     if not user_input.strip():\n",
    "#         return \"Please enter a question.\"\n",
    "    \n",
    "#     result = qa_chain({\"query\": user_input})\n",
    "#     answer = result['result']\n",
    "    \n",
    "#     # Optional: show source documents metadata\n",
    "#     sources = []\n",
    "#     for doc in result['source_documents']:\n",
    "#         sources.append(doc.metadata.get(\"source\", \"Unknown\"))\n",
    "#     sources_text = \"\\n\".join(sources)\n",
    "    \n",
    "#     return f\"Answer:\\n{answer}\\n\\nSources:\\n{sources_text}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eff1df9a-d3e4-4734-bc29-76676faa76ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# # --- Build Gradio Interface ---\n",
    "# iface = gr.Interface(\n",
    "#     fn=answer_question,\n",
    "#     inputs=gr.Textbox(lines=2, placeholder=\"Ask me anything about the docs...\"),\n",
    "#     outputs=gr.Textbox(label=\"Answer\"),\n",
    "#     title=\"AI-Powered Document Helper\",\n",
    "#     description=\"Ask questions about your Word documents (text + images). Uses RAG with fallback to OpenAI.\"\n",
    "# )\n",
    "\n",
    "# # --- Launch ---\n",
    "# iface.launch(share=True, server_name=\"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04d4f395-02c3-4ce9-ba87-dca7cfcb92e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Interactive QnA Chat within Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "328c1485-8fa7-46dc-a683-d42638ec5156",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ask a question:  How can we ingest data in DLT in Databricks?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not found in docs. Using general LLM knowledge.\n",
      "In Databricks, you can ingest data into Delta Lake (DLT) using several methods. Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark and big data workloads. Here are some common ways to ingest data into Delta Lake in Databricks:\n",
      "\n",
      "1. **Batch Ingestion**:\n",
      "   - **Spark DataFrames**: You can read data from various sources (like CSV, JSON, Parquet, etc.) into a Spark DataFrame and then write it to a Delta table using the `write` method.\n",
      "     ```python\n",
      "     df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"path/to/csv\")\n",
      "     df.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/table/path\")\n",
      "     ```\n",
      "\n",
      "2. **Streaming Ingestion**:\n",
      "   - **Structured Streaming**: You can set up a streaming DataFrame that reads from a source (like Kafka, socket, etc.) and writes to a Delta table.\n",
      "     ```python\n",
      "     streamingDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"server:port\").option(\"subscribe\", \"topic\").load()\n",
      "     query = streamingDF.writeStream.format(\"delta\").outputMode(\"append\").start(\"/delta/table/path\")\n",
      "     ```\n",
      "\n",
      "3. **Data Ingestion with Auto Loader**:\n",
      "   - **Auto Loader**: This is a feature in Databricks that allows you to incrementally and efficiently process new files as they arrive in cloud storage (like AWS S3 or Azure Blob Storage).\n",
      "     ```python\n",
      "     df = spark.readStream.format(\"cloudFiles\").option(\"cloudFiles.format\", \"csv\").load(\"path/to/directory\")\n",
      "     df.writeStream.format(\"delta\").option(\"checkpointLocation\", \"path/to/checkpoint\").start(\"/delta/table/path\")\n",
      "     ```\n",
      "\n",
      "4. **Using Databricks Jobs**:\n",
      "   - You can create a Databricks job that runs a notebook or a JAR file to ingest data into Delta Lake on a scheduled basis.\n",
      "\n",
      "5. **Using SQL Commands**:\n",
      "   - You can also use SQL commands to create Delta tables and insert data into them.\n",
      "     ```sql\n",
      "     CREATE TABLE delta_table USING DELTA LOCATION '/delta/table/path';\n",
      "     INSERT INTO delta_table SELECT * FROM another_table;\n",
      "     ```\n",
      "\n",
      "6. **Data Ingestion from External Sources**:\n",
      "   - Databricks supports various connectors to ingest data from external databases (like JDBC, Snowflake, etc.) directly into Delta Lake.\n",
      "\n",
      "7. **Using Databricks Notebooks**:\n",
      "   - You can use notebooks to run code that ingests data into Delta Lake interactively, allowing for data exploration and transformation before writing to Delta.\n",
      "\n",
      "By leveraging these methods, you can efficiently ingest data into Delta Lake in Databricks, enabling you to take advantage of its features like ACID transactions, schema enforcement, and time travel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ask a question:  What are the problems that are solved by ACID?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The problems solved by ACID include:\n",
      "\n",
      "1. Streamlined Data Append: ACID ensures efficient data appending, even with concurrent writes.\n",
      "2. Simplified Data Modification: It simplifies data modification while ensuring data consistency.\n",
      "3. Data Integrity Through Job Failures: ACID prevents data inconsistencies due to job failures, maintaining data integrity.\n",
      "4. Support for Real-time Operations: ACID provides a robust framework for real-time and streaming operations.\n",
      "5. Efficient Historical Data Version Management: ACID supports time travel for accessing historical data versions, ensuring cost-effectiveness based on specific use cases.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ask a question:  quit"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "while True:\n",
    "    question = input(\"Ask a question: \")\n",
    "    if question.lower() in [\"exit\", \"quit\"]:\n",
    "        break\n",
    "    result = smart_qa(question) #qa_chain({\"query\": question})\n",
    "    print(result)\n",
    "    # print(\"Sources:\", [d.metadata['source'] for d in result['source_documents']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30ebd3e3-c4e2-4c35-9094-5ec335669e04",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Personal_AI_Assisted_Document_Retreiver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
